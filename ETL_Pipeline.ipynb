{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Scientific Publications Data Warehouse\n",
    "\n",
    "Project 1: A Data Cube on top of Delta Lake (ETL)\n",
    "#### *Purpose*\n",
    "The purpose is to extract data about scientific publications from JSON data that describe, title, topic, authors, etc., about a large number of papers and populate a data warehouse to issue analytics queries using SQL.\n",
    "\n",
    "We will use Spark DataFrames to extract and transform the data.\n",
    "\n",
    "We will also use Spark tables (delta tables) to be used for dimensions and fact tables as will be shown below.\n",
    "\n",
    "### *DWH Schema*\n",
    "\n",
    "We will follow the proposed schema as shown:\n",
    "\n",
    "DBLP Fact Table:\n",
    "    - Date_ID (FK)\n",
    "    - Keyword_ID (FK)\n",
    "    - Type_ID (FK)\n",
    "    - Publication_ID (FK)\n",
    "    - Venue_ID (FK)\n",
    "    - FOS_ID (FK)\n",
    "    - ORG_ID (FK)\n",
    "    - Author_ID (FK)\n",
    "    - Lange_ID (FK)\n",
    "    - AuthorRank\n",
    "\n",
    "Keyword Table:\n",
    "    - ID\n",
    "    - Text\n",
    "\n",
    "Type Table:\n",
    "    - ID\n",
    "    - Description\n",
    "\n",
    "Publication Table:\n",
    "    - ID\n",
    "    - Title\n",
    "    - Year\n",
    "    - PageStart\n",
    "    - PageEnd\n",
    "    - DOI\n",
    "    - PDF\n",
    "    - URL\n",
    "    - Abstract\n",
    "    - IndexedAbstract\n",
    "    - N_Citation\n",
    "\n",
    "Venue Table:\n",
    "    - ID\n",
    "    - Name\n",
    "    - City\n",
    "    - Country\n",
    "\n",
    "Date Table:\n",
    "    - ID\n",
    "    - Year\n",
    "    - Month\n",
    "    - Day\n",
    "\n",
    "Language Table:\n",
    "    - ID\n",
    "    - Name\n",
    "\n",
    "FOS Table:\n",
    "    - ID\n",
    "    - Field\n",
    "\n",
    "ORG Table:\n",
    "    - ID\n",
    "    - Name\n",
    "    - City\n",
    "    - Country\n",
    "\n",
    "Author Table:\n",
    "    - ID\n",
    "    - FirstName\n",
    "    - LastName\n",
    "    - MiddleName\n",
    "\n",
    "\n",
    "### *Dataset*\n",
    "\n",
    "The data source is https://www.aminer.org/citation, version 13, as it is the most detailed one in JSON\n",
    "format. You can also check the schema of the respective data set on the same page under the  \"Description\" link â€“ note that the schema may not correspond to the schema in the JSON file.\n",
    "\n",
    "\n",
    "#### Dataschema of V13\n",
    "\n",
    "_Backed to v11 schema, where id and references are in String form._*\n",
    "\n",
    "\n",
    "| --- | --- | --- | ---\n",
    "| Field Name | Field Type | Description | Example\n",
    "| id | string | paper ID | 43e17f5b20f7dfbc07e8ac6e\n",
    "| title | string | paper title | Data mining: concepts and techniques\n",
    "| authors.name | string | author name | Jiawei Han\n",
    "| authors.org | string | author affiliation | Department of Computer Science, University of Illinois at Urbana-Champaign\n",
    "| authors.id | string | author ID | 53f42f36dabfaedce54dcd0c\n",
    "| venue.id | string | paper venue ID | 53e17f5b20f7dfbc07e8ac6e\n",
    "| venue.raw | string | paper venue name | Inteligencia Artificial, Revista Iberoamericana de Inteligencia Artificial\n",
    "| year | int | published year | 2000\n",
    "| keywords | list of strings | keywords | [\"data mining\", \"structured data\", \"world wide web\", \"social network\", \"relational data\"]\n",
    "| fos.name | string | paper fields of study | Web mining\n",
    "| fos.w | float | fields of study weight | 0.659690857\n",
    "| references | list of strings | paper references | [\"4909282\", \"16018031\", \"16159250\",  \"19838944\", ...]\n",
    "| n_citation | int | citation number | 40829\n",
    "| page_start | string | page start | 11\n",
    "| page_end | string | page end | 18\n",
    "| doc_type | string | paper type: journal, book title... | book\n",
    "| lang | string | detected language | en\n",
    "| publisher | string | publisher | Elsevier\n",
    "| volume | string | volume | 10\n",
    "| issue | string | issue | 29\n",
    "| issn | string | issn | 0020-7136\n",
    "| isbn | string | isbn | 1-55860-489-8\n",
    "| doi | string | doi | 10.4114/ia.v10i29.873\n",
    "| pdf | string | pdf URL | //static.aminer.org/upload/pdf/1254/ 370/239/53e9ab9eb7602d970354a97e.pdf\n",
    "| url | list | external links | [\"http://dx.doi.org/10.4114/ia.v10i29.873\", \"http://polar.lsi.uned.es/revista/index.php/ia/ article/view/479\"]\n",
    "| abstract | string | abstract | Our ability to generate...\n",
    "| indexed_abstract | dict | indexed abstract | {\"IndexLength\": 164, \"InvertedIndex\": {\"Our\": [0], \"ability\": [1], \"to\": [2, 7, ...]}}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-20 12:21:31--  https://originalstatic.aminer.cn/misc/dblp.v13.7z\r\n",
      "Resolving originalstatic.aminer.cn (originalstatic.aminer.cn)... 159.27.2.14\r\n",
      "Connecting to originalstatic.aminer.cn (originalstatic.aminer.cn)|159.27.2.14|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2568255035 (2.4G) [application/x-7z-compressed]\r\n",
      "Saving to: 'dblp.v13.7z'\r\n",
      "\r\n",
      "dblp.v13.7z         100%[===================>]   2.39G  10.9MB/s    in 4m 9s   \r\n",
      "\r\n",
      "2023-04-20 12:25:43 (9.82 MB/s) - 'dblp.v13.7z' saved [2568255035/2568255035]\r\n",
      "\r\n",
      "\r\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\r\n",
      "p7zip Version 16.02 (locale=C,Utf16=off,HugeFiles=on,64 bits,16 CPUs AMD Ryzen 9 5900HS with Radeon Graphics         (A50F00),ASM,AES-NI)\r\n",
      "\r\n",
      "Scanning the drive for archives:\r\n",
      "1 file, 2568255035 bytes (2450 MiB)\r\n",
      "\r\n",
      "Extracting archive: dblp.v13.7z\r\n",
      "--\r\n",
      "Path = dblp.v13.7z\r\n",
      "Type = 7z\r\n",
      "Physical Size = 2568255035\r\n",
      "Headers Size = 130\r\n",
      "Method = LZMA2:24\r\n",
      "Solid = -\r\n",
      "Blocks = 1\r\n",
      "\r\n",
      "Everything is Ok\r\n",
      "\r\n",
      "Size:       17352640799\r\n",
      "Compressed: 2568255035\r\n"
     ]
    }
   ],
   "source": [
    "# let's fetch the data\n",
    "# !wget https://originalstatic.aminer.cn/misc/dblp.v13.7z\n",
    "# !7z x dblp.v13.7z\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T09:29:12.122802500Z",
     "start_time": "2023-04-20T09:21:31.099174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# we will read and process the data while cleaning it simultaneously\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "# import tqdm.notebook\n",
    "\n",
    "def process_json(file_name, split_size, output_prefix, offset=0, file_number=0):\n",
    "    with open(file_name, 'r', encoding='utf-8') as ifh:\n",
    "        # seek to the second line in the file\n",
    "        if offset > 0:\n",
    "            ifh.seek(offset)\n",
    "        else:\n",
    "            ifh.seek(1) # skip the first '['\n",
    "        file_number = file_number\n",
    "        checkpoint = []\n",
    "        file_sizes = []\n",
    "        end_of_file = False\n",
    "        # we will keep looping until all the lines are read\n",
    "        while not ifh or not end_of_file:\n",
    "            file_number += 1\n",
    "            # json_objects = [ast.literal_eval(build_json_object(ifh)) for _ in tqdm.notebook.tqdm(range(split_size))]\n",
    "            json_objects = []\n",
    "            while len(json_objects) < split_size:\n",
    "                try:\n",
    "                    json_objects.append(ast.literal_eval(build_json_object(ifh)))\n",
    "                except:\n",
    "                    end_of_file = True\n",
    "                    break # we reached the end of the file\n",
    "            print(f\"Checkpoint {file_number}: {ifh.tell()}, objects processed: {len(json_objects)}\")\n",
    "            # write each json object to a file\n",
    "            with open(f\"{output_prefix}{file_number}.json\", 'w', encoding='utf-8') as ofh:\n",
    "                # this process yields smaller files than using json.dump w/ indent = 4\n",
    "                for i, json_object in enumerate(json_objects):\n",
    "                    if i == len(json_objects) - 1:\n",
    "                        ofh.write(json.dumps(json_object) + \"]\")\n",
    "                    elif i == 0:\n",
    "                        ofh.write('[' + json.dumps(json_object) + \",\")\n",
    "                    else:\n",
    "                        ofh.write(json.dumps(json_object[0]) + \",\")\n",
    "            # get the size of the file\n",
    "            file_sizes.append(os.path.getsize(f\"{output_prefix}{file_number}.json\") / 1024 / 1024)\n",
    "            checkpoint.append(ifh.tell())\n",
    "            print(f\"Checkpoint {file_number}: {checkpoint[-1]}, objects processed: {len(json_objects)}, size of file {file_number}: {file_sizes[-1]} MB\")\n",
    "            # break # for testing purposes\n",
    "        print(f\"Finished processing {file_name}, {file_number} files created.\")\n",
    "        return checkpoint, file_sizes\n",
    "\n",
    "def clean_line(line):\n",
    "    if \"NumberInt\" in line:\n",
    "        line = line.replace(\"NumberInt\", \"\") # NumberInt(123) -> (123)\n",
    "        line = line.replace(\"(\", '\"') # (123) -> \"123)\n",
    "        line = line.replace(\")\", '\"') # \"123) -> \"123\"\n",
    "    if \": null,\" in line or \": null\" in line:\n",
    "        line = line.replace(\"null\", '\"\"')\n",
    "    return line\n",
    "\n",
    "def build_json_object(fh):\n",
    "    buffer = ''\n",
    "    line = fh.readline()\n",
    "    while line != \"},\\n\":\n",
    "        if not line:\n",
    "            print(\"Reached end of file\")\n",
    "            return buffer[:-2]\n",
    "        buffer += clean_line(line)\n",
    "        line = fh.readline()\n",
    "    buffer += line\n",
    "    return buffer\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T12:42:53.330663200Z",
     "start_time": "2023-04-20T12:42:53.315020600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached end of file\n",
      "Reached end of file\n",
      "Checkpoint 54: 17352640799, objects processed: 54309\n",
      "Checkpoint 54: 17352640799, objects processed: 54309, size of file 54: 65.9983720779419 MB\n",
      "Finished processing dblpv13.json, 54 files created.\n"
     ]
    }
   ],
   "source": [
    "# results = process_json('dblpv13.json', 100000, 'clean_dataset/dblpv13_clean_', offset=17260419910, file_number=53)\n",
    "results = process_json('dblpv13.json', 100000, 'clean_dataset/dblpv13_clean_')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T12:43:04.883874700Z",
     "start_time": "2023-04-20T12:42:54.068889100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transform\n",
    "\n",
    "Here we will begin the transformation part of our pipeline. We will use delta tables and pyspark dataframes to do this. There are a few tasks that we must complete:\n",
    "1. Drop publications with very short titles (one word, empty authors, etc.)\n",
    "2. Visualize the number of citations\n",
    "3. ISSN is sometimes filled with wrong values, we can either drop or make an effor to resolve using DOI for instance.\n",
    "4. Defining the type of publication (journal, book, conference, etc.)\n",
    "5. Resolving ambiguous author names\n",
    "6. Resolving ambiguous or abbreviated conference and journal names using DBLP database.\n",
    "7. Refining venues\n",
    "8. Author gender\n",
    "9. H-index of authors\n",
    "10. Normalization of the field of study"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T16:59:48.137747800Z",
     "start_time": "2023-04-20T16:59:47.317942600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "path_to_data = 'clean_dataset/dblpv13_cleanv2_'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T16:59:48.139617Z",
     "start_time": "2023-04-20T16:59:48.126317200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 19:59:56 WARN Utils: Your hostname, LAPTOP-5Q7KN03U resolves to a loopback address: 127.0.1.1; using 172.24.98.66 instead (on interface eth0)\n",
      "23/04/20 19:59:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/jkat/miniconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jkat/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jkat/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0c69fcaf-37b6-44cf-9e39-575a37f5d324;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 672ms :: artifacts dl 33ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0c69fcaf-37b6-44cf-9e39-575a37f5d324\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 20:00:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# change the memory size depending\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Project1\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"file:///tmp/spark-warehouse\")\\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "    .config(\"spark.executor.memory\", \"16g\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"16g\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T17:00:14.267386800Z",
     "start_time": "2023-04-20T16:59:50.977403700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Function to merge two schemas\n",
    "def merge_schemas(schema1, schema2):\n",
    "    merged_fields = {field.name: field for field in schema1}\n",
    "    for field in schema2:\n",
    "        if field.name not in merged_fields:\n",
    "            merged_fields[field.name] = field\n",
    "    return StructType(sorted(merged_fields.values(), key=lambda field: field.name))\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    df = df.filter(df.title.isNotNull()) \\\n",
    "        .filter(length(df.title) > 5) \\\n",
    "        .filter(~df.title.rlike(\".*Editorial.*\")) \\\n",
    "        .filter(~df.title.rlike(\"Forward.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Preface.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Conference.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Proceedings.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Symposium.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Workshop.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Tutorial.*\")) \\\n",
    "        .filter(~df.title.rlike(\".*Forum.*\"))\n",
    "\n",
    "    df = df.filter(length(df.abstract) > 0)\n",
    "\n",
    "    df = df.filter(df.issn.isNotNull()) \\\n",
    "        .filter(length(df.issn) > 5)\n",
    "\n",
    "    df = df.filter(~df.doi.rlike(\".*[a-zA-Z]+.*\"))\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T19:02:46.250861700Z",
     "start_time": "2023-04-20T19:02:46.200145900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# We read JSON files with schema inference and preprocess them\n",
    "json_files = [f\"{path_to_data}{i}.json\" for i in range(1, 54)]\n",
    "dataframes = [preprocess_dataframe(spark.read.option(\"inferSchema\", \"true\").json(file)) for file in json_files]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T19:08:41.339930500Z",
     "start_time": "2023-04-20T19:03:29.886291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 22:14:39 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 742:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 22:21:15 WARN DAGScheduler: Broadcasting large task binary with size 1985.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Merge schemas from all dataframes\n",
    "merged_schema = dataframes[0].schema\n",
    "for df in dataframes[1:]:\n",
    "    merged_schema = merge_schemas(merged_schema, df.schema)\n",
    "\n",
    "# Apply the merged schema to all dataframes\n",
    "dataframes_with_merged_schema = [df.selectExpr(*merged_schema.fieldNames()) for df in dataframes]\n",
    "\n",
    "# Union all dataframes to create a single dataframe with a consistent schema\n",
    "combined_df = dataframes_with_merged_schema[0]\n",
    "for df in dataframes_with_merged_schema[1:]:\n",
    "    combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "# Save the combined dataframe to a Delta table\n",
    "combined_df.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/dblpv13\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T19:21:29.706537400Z",
     "start_time": "2023-04-20T19:14:37.246656600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumn(\"publication_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"author_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"venue_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"fos_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"org_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"date_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"keyword_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"type_id\", monotonically_increasing_id()) \\\n",
    "    .withColumn(\"lang_id\", monotonically_increasing_id())\n",
    "\n",
    "dblp_fact_table = combined_df.select(\"date_id\", \"keyword_id\", \"type_id\", \"publication_id\", \"venue_id\",\n",
    "                                      \"fos_id\", \"org_id\", \"author_id\", \"lang_id\")\n",
    "\n",
    "keyword_table = combined_df.select(\"keyword_id\",\n",
    "                                   col(\"keywords\").alias(\"text\"))\n",
    "\n",
    "\n",
    "venue_table = combined_df.select(\"venue_id\",\n",
    "                                 col(\"venue.name_d\").alias(\"name\"),\n",
    "                                 col(\"venue.type\").alias(\"type\"),\n",
    "                                 col(\"venue.raw\").alias(\"raw\"),\n",
    "                                 col(\"venue._id\").alias(\"vid\"))\n",
    "\n",
    "date_table = combined_df.select(\"date_id\",\n",
    "                                year(\"year\").alias(\"year\"))\n",
    "\n",
    "language_table = combined_df.select(\"lang_id\",\n",
    "                                    col(\"lang\").alias(\"name\"))\n",
    "\n",
    "\n",
    "fos_table = combined_df.select(\"fos_id\",\n",
    "                               col(\"fos\").alias(\"field\"))\n",
    "\n",
    "author_table = combined_df.select(\"author_id\",\n",
    "                                  col(\"authors.name\").alias(\"name\"),\n",
    "                                  col(\"authors.org\").alias(\"org\"),\n",
    "                                  col(\"authors.gid\").alias(\"gid\"),\n",
    "                                  col(\"authors.orgid\").alias(\"orgid\"))\n",
    "\n",
    "publication_table = combined_df.select(\"publication_id\",\n",
    "                                       col(\"title\").alias(\"name\"),\n",
    "                                       col(\"abstract\").alias(\"description\"),\n",
    "                                       col(\"doi\").alias(\"doi\"),\n",
    "                                       col(\"issn\").alias(\"issn\"),\n",
    "                                       col(\"isbn\").alias(\"isbn\"),\n",
    "                                       col(\"url\").alias(\"url\"),\n",
    "                                       col(\"pdf\").alias(\"pdf\"),\n",
    "                                       col(\"page_start\").alias(\"page_start\"),\n",
    "                                       col(\"page_end\").alias(\"page_end\"),\n",
    "                                       col(\"volume\").alias(\"volume\"),\n",
    "                                       col(\"issue\").alias(\"issue\"),\n",
    "                                       col(\"n_citation\").alias(\"n_citation\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T19:48:17.861229300Z",
     "start_time": "2023-04-20T19:48:16.856601500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 855:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 22:55:39 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 968:===================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:02:41 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1081:==================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:09:54 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1194:==================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:16:57 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1307:==================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:24:07 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1420:==================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:31:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1532:=============>(15 + 1) / 16][Stage 1533:=============>(15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:39:13 WARN DAGScheduler: Broadcasting large task binary with size 1893.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1646:==================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 23:48:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dblp_fact_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/dblp_fact_table\")\n",
    "keyword_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/keyword_table\")\n",
    "venue_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/venue_table\")\n",
    "date_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/date_table\")\n",
    "language_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/language_table\")\n",
    "fos_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/fos_table\")\n",
    "author_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/author_table\")\n",
    "publication_table.write.format(\"delta\").mode(\"append\").save(\"clean_dataset/delta/publication_table\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-04-20T20:48:38.190760300Z",
     "start_time": "2023-04-20T19:48:52.811099300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load table from delta lake\n",
    "dblp_fact_table = spark.read.format(\"delta\").load(\"clean_dataset/delta/dblp_fact_table\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
